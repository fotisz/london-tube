---
title: "London Underground"
author: "Ruaridh Williamson"
output:
  html_document:
    toc: true
---

# Welcome

Transport for London's Open Data repository was recently valued by Deloitte
as providing up to [Â£130mil of economic benefits][1] for the City of London by
allowing 600 third-party transport apps access to real-time transit information
via a common Application Programming Interface.

[1]: https://tfl.gov.uk/info-for/media/press-releases/2017/october/tfl-s-free-open-data-boosts-london-s-economy

This project is split into two parts which tackle various analytical methods
and visualisations applied to a segment of this API; network timetabling data.

## Part One

Prior to any form of analysis, we need a structured, rectangular dataset.
The first part of the project involves building a reproducible workflow for
refreshing the timetable API feed which is updated on a weekly basis.
It starts with examining the structure of the data source before
extracting all of the relevant data into a "tidy" and normalised format
ready for pushing to a PostgreSQL database for further analysis.
For that reason, this part is written in R for the data manipulation
and PostgreSQL for in-database transformation.

For illustration purposes, only London Underground timetables are extracted
and processed however the methods presented here extend to any tube, train,
bus, ferry or tram timetable across Great Britain as the extraction process
is built around a generic XML schema used by the Department of Transport.

## Part Two

Once the data has been extracted, cleaned and normalised, the
second part looks at different ways to visualise network timetable data
and apply journey planner algorithms to calculate the fastest routes
to get across London. As with any data science project, the vast majority
of the time is spent performing tasks from Part One however equal weight
has been given to each in the resulting notebooks.

## Context

This project was completed in partial fulfilment of a course for the MSc Analytics and
Operations Research at the London School of Economics. Any analysis presented here
was designed foremost to align to the course principles and methodologies covered.

The requirements of the course are to provide the analysis in the form of exploratory
notebooks (Part One and Two respectively) however in the interests of full reproducibility
of the project all additional analysis files are included with a script outlining
the overall workflow.

The course content covered in Part One is

- Using data from the Internet, parsing XML web data (Week 4)
- Working with APIs and authentication (Week 5)
- Manipulation, transformation and extraction of data into "tidy" form (Week 2)
- Relational databases, normalisation and data integrity (Week 3)

Part Two looks at

- Exploratory data visualisation (Weeks 6 and 7)
- Graph network algorithms and visualisation (Week 11)

The content from Week 1

- Version control, website publishing and GitHub pages

is inherent in the publication of this website.







