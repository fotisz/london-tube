---
title: 'Part One: Managing Data'
output: github_document
---

[Back to Home](index.html)

```{r knitr_setup, include=FALSE}
knitr::opts_chunk$set(cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
# read_chunk() now supports SQL chunks in external files
if(packageVersion("knitr") < "1.17.18")
  stop("Require development version of knitr from GitHub for latest Rmd functionality")
```

# Introduction

This notebook forms the first of two outputs in the analysis of
the TfL London Tube network. It is primarily concerned with the
transformation and manipulation of the required datasets in
comparison to Part Two which focuses on visualisation.

Due to the complexity of the dataset being investigated, there
are numerous links to external analysis files provided which go
into greater detail regarding the transformations and data extraction
performed which do not form part of the notebook.
Further, the data manipulation code shown here is
wholly R however when it came to extracting the data
in practice this was re-written in Python for performance reasons.
Therefore this code is to illustrate the initial extraction and data
discovery thought-process moreso than the efficient extraction process.

# APIs and Internet Data

```{r get_data, include=FALSE, eval=TRUE, cache=FALSE}
knitr::read_chunk(file.path(rprojroot::find_rstudio_root_file(), "2_analysis/r/GetData.R"))
```

We start the project with a reproducible way of pulling the dataset
from the API download link. The results shown in this notebook relate
to the API feed as at 12th December 2017.

To access it, download the zip file at the link below

```{r proj_setup}
```

```{r download_zip, eval=FALSE}
```

*Note: You may need to [register](https://api-portal.tfl.gov.uk/) for an API key to
access the zip file. In the event that you cannot gain access there is a small demo sample [here](https://tfl.gov.uk/cdn/static/cms/documents/journey-planner-timetables.zip)*

Unpack the download once to split out the train/ferry timetables
from the bus timetables and again to unpack the tube/DLR timetables.

```{r unzip, eval=FALSE}
```

```{r show_xml_files}
```

The result is 600-800MB of XML files containing the most up-to-date
timetable information for the London Underground and DLR services.
The pattern of London Underground files we require are XML files that
start with "tfl_1" as the files with other prefixes are ferry, DLR or TfL rail timetables.

# Data Exploration and Transformation

For this section, we'll investigate the structure of the API feed,
ways to extract the important data, manipulate it into the required
shape and add any necessary calculations. This will be demonstrated
for one of the XML files in a manner which can be generalised for
any other timetable.

## XML Metadata

Begin by reading in the first XML file.

*NB: I have specifically refrained from loading the `xml2`, `XML` and
`xmltools` namespaces via `library` throughout in order to illustrate
which functions belong to which packages more clearly by using `::` for
each function call.*

```{r}
library(magrittr)
doc <- data.files[1] %>%
  xml2::read_xml()
doc
```

This metadata contains two important pieces of information:

1. The top level node contains *eight* distinct child nodes
which appear to be completely disparate datasets all relating to this
particular file:

  - The [*StopPoints*][1] provide the stop locations which
group up to a given stop "locality" in [*NptgLocalities*][2].
  - The *Routes* are broken down into *RouteSections* and *JourneyPatternSections*
  - The *Operator* is not useful for the Tube data as there is only one; "LUL"
  - Each *Service* is how the file derives its name so there's one in each
XML file
  - The *VehicleJourney* is the most granular level of information
as it provides the exact trip link of a vehicle moving from one station
to the next in a given *JourneyPatternSection*

2. The namespace of the XML file is non-standard.
The `schemaLocation` attribute suggests that the XML structure adheres to
the [*TransXChange*][3] schema
provided by the UK Department for Transport for timetable data.
We can configure this namespace for use with any future XPaths
(the alternative is to simply strip it entirely from the file however
this takes additional processing time).

[1]: https://en.wikipedia.org/wiki/NaPTAN#NaPTAN_Stops
[2]: https://www.gov.uk/government/publications/national-public-transport-gazetteer
[3]: https://www.gov.uk/government/collections/transxchange

```{r namespace}
namespace <- c(txc = "http://www.transxchange.org.uk/")
```

Once we set up an extraction procedure for this schema and namespace
it can then be applied to *any* timetable that adheres to DfT standards.

## Dataset Structure

To quickly get an idea of the structure we can traverse the document as a list:

```{r}
xml_list <- doc %>%
  xml2::as_list()
```

and inspect the structure of a given observation for each of the 8
tables along with an example
of the data that each XML node (or attribute) contains.

```{r}
str(xml_list$NptgLocalities$AnnotatedNptgLocalityRef)
str(xml_list$StopPoints$StopPoint)
str(xml_list$RouteSections$RouteSection[[1]])
str(xml_list$Routes$Route)
str(xml_list$JourneyPatternSections$JourneyPatternSection[[1]])
str(xml_list$Operators$Operator)
str(xml_list$Services$Service$StandardService$JourneyPattern)
str(xml_list$VehicleJourneys$VehicleJourney)

# Show service data without the child node JourneyPatterns
service <- xml_list$Services$Service
service$StandardService <- NULL
str(service)
```

Four of the tables contain particularly crucial information other than Reference
fields and linking identifiers:

**Stop Points**

A tube station at which a given vehicle can stop.

- *AtcoCode* is a unique identifier of the stop across the network. There
is one for the inbound stops and a second identifier for the same stop
going the other way. These group up into *Localities* (under the *Place* node)
- *CommonName* is the stop's name
- *Location* contains its *Easting* and *Northing* coordinates

**Journey Pattern Sections**

A trip link between two **Stop Points** (*From* and *To*) with a given
*RunTime* and optional *WaitTime*. These combine to an overall **Journey Pattern**.

- *From* and *To* fields show the trip links between two stations
- *WaitTime* is encoded as "PT1M" for example (assumed to be 1 minute of waiting)
however is not applicable to every trip link (therefore the XML tag often disappears!)
- *RunTime* is also encoded and provides the time taken for a trip link

**Services**

Provides information about the operating rhythm of the timetable

- *LineName* though this is fairly obvious from the file name
- *Operating Period* as this tells us which period of time the timetable
is actually active for. This particular timetable is only in effect
on Christmas Eve.
- *Operating Profile* says for which day(s) of the week and bank holidays
the timetable is and isn't operational. Again, these tags will vary in their
structure from document to document making these fields quite difficult to parse.

**VehicleJourneys**

A tube vehicle's *Journey Pattern* and the time of departure from its origin
station.

- *Operating Profile* again as some journeys may only apply to some of the days
that a service spans
- *Departure Time* provides the train *origin* departure time. The difficult part
is to now interpolate the departure time of *each station* along a journey
using the RunTime and WaitTime in **Journey Pattern Sections**.

It's also worth noting that the **StopPoints** and **NptgLocalities** tables both
contain duplicate information across every **Service** (XML file) for a given line and
also contain duplicates *across* lines that share the same stops. Every other
table has unique information per XML file because the routes, trip links,
vehicles and journeys are all defined by Service.
This would mean that the timetable for Christmas Day has
a completely different set of trip links to that of Boxing Day (despite the trains
physically running on exactly the same journey patterns) purely because there is
a slight variation in the frequency of Vehicle Journeys between the dates.
This clearly leads to a vast
amount of duplicated information which can be easily cut down by a factor of up to 40x
by extracting a subset of timetables of interest.

For a comprehensive description of the schema if not to appreciate the scale
and complexity of the dataset, see the official DfT documentation
[here](http://naptan.dft.gov.uk/transxchange/schema/2.5/doc/TransXChangeSchemaGuide-2.5-v-59.pdf). Page 63 in particular details how many of the elements discussed
above will be joined in a PostgreSQL database later in this notebook.

## Subsetting

As seen in the previous section, the entire file shown is only relevant for
one day of the year. By first filtering the files based on the Operating
Period and Operating Profile, it's possible to only read in the files
that relate to a Monday or New Years Day or 17th January.

Here is a table containing every file along with its starting and ending
operating period dates:

```{r extract_op_period, warning=FALSE, message=FALSE}
library(data.table)
xpath.op_period <- ".//txc:Services/txc:Service/txc:OperatingPeriod/*"

dates <- sapply(data.files, function(xml_file) {
  XML::xmlParse(xml_file) %>%
    XML::xmlRoot() %>%
    XML::xpathApply(xpath.op_period, namespaces = namespace,
                    fun = function(y) XML::xmlValue(y) %>% as.Date)
}, simplify = FALSE) %>%
  {data.table(names(.), rbindlist(.))}
names(dates) <- c("File", "StartDate", "EndDate")
dates[["File"]] <- basename(dates[["File"]])
head(dates, 10)
```

The next step is to filter this dataframe based on whether a date of
interest falls within the time interval of a file:

```{r, warning=FALSE, message=FALSE}
library(lubridate)
timetable_date <- lubridate::today()
relevant_dates <- dates %$%
  # Test whether the timetable_date falls within StartDate to EndDate
  interval(StartDate, EndDate) %>%
  {timetable_date %within% .} %>%
  # Subset dates to only these records
  dates[.]
head(relevant_dates, 10)
```

Now we've cut down the 84 files to roughly a quarter depending on the
chosen `timetable_date`.

The other option (to filter based on the days of the week) is
unfortunately even more convoluted.
It should be as simply as filtering on the tags underneath
Journey Operating Profile however here's a list of the unique set of tags:

```{r subset_given_day}
xpath.days_of_week <- ".//txc:VehicleJourneys/txc:VehicleJourney/txc:OperatingProfile/txc:RegularDayType/txc:DaysOfWeek/*"
daysofweek <- sapply(data.files, function(xml_file) {
  XML::xmlParse(xml_file) %>%
    XML::xmlRoot() %>%
    XML::xpathApply(xpath.days_of_week, namespaces = namespace, fun = XML::xmlName)
}, simplify = FALSE) %>%
  unlist %>%
  unique
daysofweek
```

The "days of the week" also include values such as "MondayToFriday",
"Weekend" as well as "MondayToSaturday" and "MondayToSunday" under the
Service Operating Profile. So we would need to create a mapping function
between a given date and all of the possible "days of the week" it could
fall under.
As an example of how this can be accounted for, see
[this PostgreSQL table](DaysOfWeekGroups.html).

## Extraction

```{r extraction, include=FALSE, eval=TRUE, cache=FALSE}
knitr::read_chunk(file.path(rprojroot::find_rstudio_root_file(), "2_analysis/r/GetTablesFromXPaths.R"))
```

Now that we have a good understanding of what each file contains, we can
attempt to extract the key information from one of them before generalising
this process for all files. As we've already seen, the tree structure of
these XML files is particularly complex thereby making one-size-fits-all functions
such as `xmlToDataFrame` rather unhelpful. The level of control required
is really only achievable by defining XPaths for the location of each piece
of information we need and then looping through the document extracting every
instance of each path. On the plus side, the files are well-formed and do not
contain any errors which we may find in malformed web data.

Firstly, we'll want to retrieve the parent XPaths for each of the nodes
containing useful information. This is done simplest with a helper function
`xml_get_paths` from the [xmltools](https://github.com/dantonnoriega/xmltools) library.

<!--
The empty code chunks represent code that lives in external files.
If you wish to execute the chunks interactively, run the corresponding
`read_chunk` (see previous chunk) to source the code and execute it with
`eval(parse(text=knitr:::knit_code$get()$extraction_setup))`
 -->

```{r extraction_setup}
```
```{r print_xpaths}
head(terminal_xpaths)
```

These are the XPaths to every possible terminal branch of the tree however a lot
of the information isn't particularly useful. For some of the tables,
we're only interested in specific sections. For example, the *Operational*
tag isn't interesting as it contains information about the vehicle which
in this case will always be "Underground Train". This is a similar story
for the *Operator* which is always "London Underground".

Therefore we can define a subset of the `terminal_xpaths` grouped by the table
to which they each relate.

```{r required_xpaths}
```
```{r print_required_xpaths}
terminal_xpaths[unlist(required_xpaths)]
```

This way, when it comes to extracting the data, all of the information
found under the 8th, 9th and 10th paths of `terminal_xpaths` will be
combined together under the *RouteSections* table (see elements 6-8 above).

Now it is just a matter of looping over this list, finding all of the
matches for each path, extracting the underlying data and combining it
into a single dataset for each of the tables.

```{r build_tfl}
```
```{r print_tfl}
tfl <- build_tfl(doc, terminal_xpaths, required_xpaths)
str(tfl, 1)
```

The result is the `tfl` list which contains the eight tables corresponding
to the top-level nodes of this single XML file.

Due to the flexibility of XML and the TransXChange schema however, there's
still a lot missing...

1. A number of important fields are found in the "id" *attribute* of certain tags,
not in the tag's text as we might expect.
1. The stop sequence numbers are contained in a "SequenceNumber" attribute
1. Some tables have a parent which contains join information. For example, the
JourneyPatterns table is defined at the level of each Journey Pattern Section
however we are missing the foreign key Journey Pattern ID as this is the parent node
of each Section and therefore unable to be parsed with the same XPath.

We can define a function `retrieve_additional_fields` which solves these three
cases and call it after `build_tfl` for each document. Due to its length and
complexity it is not shown here but can be found in the
[complete analysis file](GetTablesFromXPaths.html).

```{r retrieve_additional_fields, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
```
```{r run_retrieve_additional_fields}
library(magrittr)
tfl %<>% retrieve_additional_fields(doc)
```

We can put the whole extraction process together by calling `read_xml`,
`build_tfl` and `retrieve_additional_fields` for each XML file:

```{r extract_file}
```

and then loop this function over the entire directory of files:

```{r run_extract_file, eval=FALSE}
tfl_all <- purrr::map(data.files, extract_file)
```

Unfortunately, the bad news is this line of code takes a **very** long time
to run so don't execute it unless you plan on waiting all night. Literally...

Despite a few good libraries, R is pretty shocking at XML scraping
when compared to the `lxml` Python library. The versatility of `lxml`
is also significantly better when it comes to traversing XML trees
efficiently and extracting data in nested nodes, parent nodes and attributes
as we require.

To cope with this problem, the dataset was instead scraped (with even more
fields in more hard-to-reach places and about 500x faster) with
[this file](XMLParsing.html) which calls a custom-built [TfL Class](TfLTimetable.html).

## Variable Creation

```{r variable_creation, include=FALSE, eval=TRUE, cache=FALSE}
knitr::read_chunk(file.path(rprojroot::find_rstudio_root_file(), "2_analysis/r/VariableCreation.R"))
```

There are a few crucial variables which we'll need to add along with correctly
typecasting the current information for any further analysis.
This is best done in R before handing over to a database.
Due to the potential scale of the join operations (had we not already
subsetted the dataset), additional variables which are calculations *across*
tables will be left to the database stage. This is also to preserve the
normalised nature of the tables within the database.

### Latitude and Longitude

To make any use of the station locations, it's necessary to convert the provided
data to a more renowned coordinate system.

The [TransXChange website](http://naptan.dft.gov.uk/transxchange/technicalFaq.htm#PubCoords)
confirms that the "Easting" and "Northing" fields in the StopPoints dataset are
British National Grid (BNG) coordinates whereas we require the more popular latitude and longitude.

This can be achieved with the R Geospatial Data Abstraction Library which is
able to convert between the two coordinate projections via the
[PROJ.4](http://proj4.org/) library:

```{r modify_StopPoints}
```

### JourneyTime and DepartureMins

Perhaps the most significant variable is the RunTime of each journey section
as it supposedly tells us the travel time between two StopPoints.

```{r runtime_frequency}
table(tfl$JourneyPatternTimingLinks$RunTime)
```

Unfortunately this frequency table shows that the duration is presented as a text field
where it's assumed the "M" indicates the RunTime is rounded to the nearest minute.
Judging by the "0S" field, this would also suggest that a number of links
take zero seconds!
Clearly the duration has been rounded to the nearest minute however it also
turns out that every instance of zero RunTime has at least 1 minute of WaitTime at the
next station.

```{r check_waittime_runtime}
# Check that there aren't records with both no WaitTime and zero RunTime
tfl$JourneyPatternTimingLinks %>%
  filter(is.na(WaitTime) && RunTime == "PT0S") %>%
  nrow == 0
```

Therefore, we can convert the RunTime and WaitTime to integer variables by extracting
the third character from every observation and build a new variable "JourneyTime" which
takes into account the total RunTime and WaitTime which we now know will always
be at least 1 minute:

```{r modify_JourneyPatternTimingLinks}
```

The other most important field is the departure times of every train from
their origin station, DepartureTime, which is currently stored as text.

```{r departuretime_frequency}
head(tfl$VehicleJourneys$DepartureTime)
```

So it looks like this variable is a time stamp which is also rounded to the nearest minute.

Since we are now dealing solely in minutes on a day-by-day basis,
it would make the most sense to cast
this DepartureTime field as "the number of minutes since midnight". This makes it less
human-readable however addition of DepartureTime with JourneyTime is now very
straightforward and doesn't rely on date-time typecasting or datediff functions.
That said, in the event that we're using a database that handles date-times well,
it doesn't hurt to correctly cast DepartureTime and keep it in the data frame
separately.

```{r modify_VehicleJourneys}
```

# Normalisation and Relational Databases

```{r sqltables, include=FALSE, eval=TRUE, cache=FALSE}
knitr::read_chunk(file.path(rprojroot::find_rstudio_root_file(), "2_analysis/r/SQLTables.R"))
```

As found in the previous section, the dataset is provided in 3rd normal form
already making it very easy to simply push the tables as-is to a database.

## DBMS Setup

Due to the information given to us, the queries to be computed in-database are
quite advanced and require running cumulative calculations and row offsets by group
which are only available in an advanced DBMS and not SQLite or MySQL.

For this reason, we'll use an open-source database PostgreSQL for its
[windowing functions](http://www.postgresqltutorial.com/postgresql-window-function/)
though other proprietary databases such as Oracle or
Microsoft also provide this functionality.
Thanks to [Docker](https://www.docker.com/what-docker) containers,
provisioning a clean lightweight Postgres database on your local machine is one
line of code without any additional setup other than downloading Docker itself.

```{bash, eval=FALSE}
docker run --name postgres_london_tube -p 5432:5432 -d -e POSTGRES_PASSWORD=mysecretpassword postgres:alpine
```

*The alpine image is about 1/10th the size of the official postgres container as it uses a significantly smaller distribution of Linux with fewer of PostgreSQL's auxilliary features.*

Test that the server is running and the connection works by setting up a development
database for the project:

```{r load_rpostgresql, warning=FALSE, message=FALSE, echo=FALSE}
library(RPostgreSQL)
```

```{r create_db, eval=FALSE}
```

```{r get_con}
```

## Build database

We can write a function that loops over all eight tables sequentially
adding them to the database whilst dropping any existing tables.

```{r create_tables}
```

Finally we can check that the tables were loaded successfully:

```{r list_db_tables}
con <- get_con()
dbListTables(con)
dbDisconnect(con)
```

## Joining Tables

```{r DepartureBoard, include=FALSE, eval=TRUE, cache=FALSE}
knitr::read_chunk(file.path(rprojroot::find_rstudio_root_file(), "2_analysis/sql/DepartureBoard.sql"))
```

To build out the timetable, we've seen that there are two crucial tables:

- **VehicleJourneys** which lists each train and its time of departure from the journey origin
- **JourneyPatternTimingLinks** contains the sequence of stops for each possible journey and the
journey time of each link

What we're after is a network-wide departures board which has the time of departure from
each *station* and not just the train origin. Therefore, it's a matter of expanding out every
Vehicle Journey by the number of sequences in the Journey Pattern to calculate each Vehicle Link
departure.

The departures board table is achieved as follows:

1. Join **VehicleJourneys** to **JourneyPatternTimingLinks** via **JourneyPatterns**
1. Calculate *ArrivalMins_Link* (arrival time of each train into each station)
as the origin departure time (*DepartureMins*)
plus the cumulative sum of *JourneyTime* ordered by link sequence for each vehicle
1. Calculate *DepartureMins_Link* as the preceding link's arrival time.
For the first link, it's the origin departure time.

For analysis later on, we also want to flag whether the link is the last trip in the
vehicle's journey. This just involves checking whether the link sequence is the largest
value for that vehicle.

The table-valued function `departureboard` is as follows:

```{sql sql_departures, eval=FALSE}
```

As you can imagine, when pushing every timetable to the database
this table blows out to 6.63M rows.
In fact, as discussed earlier, the vast majority of it is redundant information.
Entire XML files are duplicated for special calendar days such as
Bank Holidays, New Years, Christmas etc. hence the need to sample the data.

If using the Python methodology of scraping the dataset for all files, the
sampling process discussed earlier now occurs as part of the Departures Board query
`WHERE` clause.

Essentially we just filter on the Operating Profile of the Services and Vehicle
Journeys to be one day of the week and filter the Operating Periods to be
those that contain a given date.
Now, by passing in the date "Wednesday 20th Dec 2017" for examplex the table size has reduced
to 244K rows.

As we now have the dataset in a cleaned and useable format, this completes the
Managing Data notebook.
We will now go on to visualise this newly created Departure Board table
in [Part Two](VisualisingData.html)
